{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qX_OAYz1WhOO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XREOVFcNWhOQ"
      },
      "outputs": [],
      "source": [
        "def load_zipped_pickle(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        loaded_object = pickle.load(f)\n",
        "        return loaded_object\n",
        "\n",
        "def save_zipped_pickle(obj, filename):\n",
        "    with gzip.open(filename, 'wb') as f:\n",
        "        pickle.dump(obj, f, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF0bU-RnW5pv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGQAMuB1WhOR"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UON5DHKuWhOT"
      },
      "outputs": [],
      "source": [
        "processed_data = load_zipped_pickle(\"/content/drive/MyDrive/data/train_processed.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoG75O2e-_LF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_class_frequency(labels):\n",
        "    \"\"\"\n",
        "    Calculate the frequency of each class in the given labels.\n",
        "    Assuming binary classes with 0 representing the background and 1 the foreground.\n",
        "    \"\"\"\n",
        "    background_count = 0\n",
        "    foreground_count = 0\n",
        "\n",
        "    # Iterate through each label (mask)\n",
        "    for label in labels:\n",
        "        # Flatten the label mask\n",
        "        label_flat = label.flatten()\n",
        "\n",
        "        # Count number of pixels for each class\n",
        "        background_count += np.count_nonzero(label_flat == 0)\n",
        "        foreground_count += np.count_nonzero(label_flat == 1)\n",
        "\n",
        "    return background_count, foreground_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCxg0S17WhOb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "videos = [entry['video'] for entry in processed_data]\n",
        "labels = [entry['label'] for entry in processed_data]\n",
        "\n",
        "height = 256\n",
        "width = 256\n",
        "\n",
        "X_frames = []\n",
        "y_labels = []\n",
        "\n",
        "for video, label in zip(videos, labels):\n",
        "    num_frames = video.shape[-1]\n",
        "    for i in range(num_frames):\n",
        "        frame = video[:, :, i]\n",
        "        label_frame = label[:, :, i]\n",
        "\n",
        "        X_frames.append(frame)\n",
        "        y_labels.append(label_frame)\n",
        "\n",
        "X = np.array(X_frames).reshape(-1, height, width, 1)  # Adding channel dimension\n",
        "y = np.array(y_labels).reshape(-1, height, width, 1)  # Adding channel dimension\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPNcp35No2yO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.ndimage import gaussian_filter, map_coordinates\n",
        "from skimage import transform\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def elastic_transform(image, label, alpha, sigma):\n",
        "    random_state = np.random.RandomState(42)\n",
        "    shape = image.shape\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing=\"ij\")\n",
        "    indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1))\n",
        "    distorted_image = map_coordinates(image, indices, order=1).reshape(shape)\n",
        "    distorted_label = map_coordinates(label, indices, order=0).reshape(shape)\n",
        "    return distorted_image, distorted_label\n",
        "\n",
        "def apply_augmentation(image, label, image_size=(256, 256)):\n",
        "    image = np.uint8(image * 255)\n",
        "    label = np.uint8(label * 255)\n",
        "\n",
        "    original_shape = image.shape[:2]\n",
        "\n",
        "    # Apply elastic deformation with a certain probability\n",
        "    if np.random.rand() < 0.1:\n",
        "        alpha_range = (1, 2)\n",
        "        sigma = 6\n",
        "        alpha = np.random.uniform(*alpha_range)\n",
        "        image, label = elastic_transform(image, label, alpha, sigma)\n",
        "\n",
        "    # Apply scaling with a 50% probability\n",
        "    if np.random.rand() < 0.5:\n",
        "        scale_factor = np.random.uniform(0.8, 1.2)\n",
        "        image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n",
        "        label = cv2.resize(label, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_NEAREST)\n",
        "        image = cv2.resize(image, image_size, interpolation=cv2.INTER_LINEAR)\n",
        "        label = cv2.resize(label, image_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Apply rotation with a 30% probability\n",
        "    if np.random.rand() < 0.3:\n",
        "        angle = np.random.uniform(0, 30)\n",
        "        center = (original_shape[1] // 2, original_shape[0] // 2)\n",
        "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "        image = cv2.warpAffine(image, rotation_matrix, original_shape)\n",
        "        label = cv2.warpAffine(label, rotation_matrix, original_shape)\n",
        "\n",
        "    # Apply flipping with a 30% probability\n",
        "    if np.random.rand() < 0.3:\n",
        "        flip_horizontal = np.random.rand() < 0.5\n",
        "        if flip_horizontal:\n",
        "            image = cv2.flip(image, 1)  # Horizontal flip\n",
        "            label = cv2.flip(label, 1)\n",
        "        else:\n",
        "            image = cv2.flip(image, 0)  # Vertical flip\n",
        "            label = cv2.flip(label, 0)\n",
        "\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    label = label.astype(np.float32) / 255.0\n",
        "    image = image.reshape(image_size + (1,))\n",
        "    label = label.reshape(image_size + (1,))\n",
        "\n",
        "    return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdytZi6Jo1nc"
      },
      "outputs": [],
      "source": [
        "# Apply augmentations with frequency adjustment to the training data\n",
        "X_train_augmented = []\n",
        "y_train_augmented = []\n",
        "\n",
        "for frame, label in zip(X_train, y_train):\n",
        "    # Apply augmentation with a given probability\n",
        "    frame = np.squeeze(frame)\n",
        "    label = np.squeeze(label)\n",
        "    augmented_frame, augmented_label = apply_augmentation(frame, label)\n",
        "\n",
        "    augmented_frame = np.expand_dims(augmented_frame.astype(np.float32), axis=-1)\n",
        "    augmented_label = np.expand_dims(augmented_label.astype(np.float32), axis=-1)\n",
        "\n",
        "    X_train_augmented.append(augmented_frame)\n",
        "    y_train_augmented.append(augmented_label)\n",
        "\n",
        "# Convert augmented lists to numpy arrays\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lFuJ0NF_Grd"
      },
      "outputs": [],
      "source": [
        "# Assuming y_train is your array of training labels (masks)\n",
        "background_count, foreground_count = calculate_class_frequency(y_train_augmented)\n",
        "\n",
        "print(f\"Background Pixels: {background_count}\")\n",
        "print(f\"Foreground Pixels: {foreground_count}\")\n",
        "total_pixels = background_count + foreground_count\n",
        "\n",
        "# Calculate weights\n",
        "weight_for_background = (1 / background_count) * (total_pixels) / 2.0\n",
        "weight_for_foreground = (1 / foreground_count) * (total_pixels) / 2.0\n",
        "\n",
        "class_weights = {0: weight_for_background, 1: weight_for_foreground}\n",
        "print(class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_h3Xgupwge-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "total_weight = weight_for_background + weight_for_foreground\n",
        "weight_for_background = weight_for_background / total_weight\n",
        "weight_for_foreground = weight_for_foreground / total_weight\n",
        "\n",
        "print(\"Weight for Background:\", weight_for_background)\n",
        "print(\"Weight for Foreground:\", weight_for_foreground)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHskamYcqa0i"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_processed_frame(frame_index, X_data, y_data):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Original image and label\n",
        "    original_image = X_data[frame_index].squeeze()\n",
        "    original_label = y_data[frame_index].squeeze()\n",
        "\n",
        "    # Plot original image\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.imshow(original_image, cmap='gray')\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot original label\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.imshow(original_label, cmap='gray')\n",
        "    plt.title('Original Label')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Apply augmentation\n",
        "    augmented_image, augmented_label = apply_augmentation(original_image, original_label)\n",
        "\n",
        "    # Plot augmented image\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.imshow(augmented_image.squeeze(), cmap='gray')\n",
        "    plt.title('Augmented Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot augmented label\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.imshow(augmented_label.squeeze(), cmap='gray')\n",
        "    plt.title('Augmented Label')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "frame_index = 100\n",
        "plot_processed_frame(frame_index, X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqC2TJ7fWhOZ"
      },
      "source": [
        "## Model REB Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq4l0x2wWhOd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, UpSampling2D, concatenate, GlobalAveragePooling2D, Dense, Reshape, multiply, Add\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "    # Apply two convolutional layers with ReLU activation and batch normalization\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def residual_block(input_tensor, num_filters):\n",
        "    # Residual block with skip connection\n",
        "    conv = conv_block(input_tensor, num_filters)\n",
        "    skip = Conv2D(num_filters, (1, 1), padding='same')(input_tensor)\n",
        "    skip = BatchNormalization()(skip)\n",
        "    output = Add()([conv, skip])\n",
        "    return output\n",
        "\n",
        "def SEBlock(input_tensor, num_filters, ratio=16):\n",
        "    # Squeeze and Excitation Block\n",
        "    squeeze = GlobalAveragePooling2D()(input_tensor)\n",
        "    excitation = Dense(num_filters // ratio, activation='relu')(squeeze)\n",
        "    excitation = Dense(num_filters, activation='sigmoid')(excitation)\n",
        "    excitation = Reshape((1, 1, num_filters))(excitation)\n",
        "    scale = multiply([input_tensor, excitation])\n",
        "    return scale\n",
        "\n",
        "def ASPP(input_tensor, num_filters, rate=[6, 12, 18]):\n",
        "    # ASPP block with different dilation rates\n",
        "    aspp1 = Conv2D(num_filters, (3, 3), dilation_rate=rate[0], padding='same')(input_tensor)\n",
        "    aspp1 = BatchNormalization()(aspp1)\n",
        "    aspp1 = Activation('relu')(aspp1)\n",
        "\n",
        "    aspp2 = Conv2D(num_filters, (3, 3), dilation_rate=rate[1], padding='same')(input_tensor)\n",
        "    aspp2 = BatchNormalization()(aspp2)\n",
        "    aspp2 = Activation('relu')(aspp2)\n",
        "\n",
        "    aspp3 = Conv2D(num_filters, (3, 3), dilation_rate=rate[2], padding='same')(input_tensor)\n",
        "    aspp3 = BatchNormalization()(aspp3)\n",
        "    aspp3 = Activation('relu')(aspp3)\n",
        "\n",
        "    # Concatenate the atrous convolution outputs\n",
        "    aspp = concatenate([aspp1, aspp2, aspp3], axis=-1)\n",
        "\n",
        "    # 1x1 convolution to combine features\n",
        "    aspp = Conv2D(num_filters, (1, 1), padding='same')(aspp)\n",
        "    aspp = BatchNormalization()(aspp)\n",
        "    aspp = Activation('relu')(aspp)\n",
        "\n",
        "    return aspp\n",
        "\n",
        "\n",
        "def create_model(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder with residual and SE blocks\n",
        "    x = residual_block(inputs, 64)\n",
        "    x = SEBlock(x, 64)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 128)\n",
        "    x = SEBlock(x, 128)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 256)\n",
        "    x = SEBlock(x, 256)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Bridge with ASPP\n",
        "    x = ASPP(x, 512)\n",
        "\n",
        "    # Decoder with residual and SE blocks\n",
        "    x = UpSampling2D((2, 2))(x)  # Upsampling to 64x64\n",
        "    x = residual_block(x, 256)\n",
        "    x = SEBlock(x, 256)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)  # Upsampling to 128x128\n",
        "    x = residual_block(x, 128)\n",
        "    x = SEBlock(x, 128)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)  # Upsampling to 256x256\n",
        "    x = residual_block(x, 64)\n",
        "    x = SEBlock(x, 64)\n",
        "\n",
        "    # Final Convolution Layer\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)  # Final layer with shape (256, 256, 1)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, UpSampling2D, concatenate, GlobalAveragePooling2D, Dense, Reshape, multiply\n",
        "\n",
        "def conv_block(input_tensor, num_filters, use_BN=True):\n",
        "    # First layer\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
        "    if use_BN:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    # Second layer\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
        "    if use_BN:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(input_tensor, num_filters, use_BN=True):\n",
        "    x = conv_block(input_tensor, num_filters, use_BN)\n",
        "    p = MaxPooling2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters, use_BN=True):\n",
        "    x = UpSampling2D((2, 2))(input_tensor)\n",
        "    x = concatenate([x, concat_tensor])\n",
        "    x = conv_block(x, num_filters, use_BN)\n",
        "    return x\n",
        "\n",
        "def SEBlock(input_tensor, num_filters, ratio=16):\n",
        "    squeeze = GlobalAveragePooling2D()(input_tensor)\n",
        "    excitation = Dense(num_filters // ratio, activation='relu')(squeeze)\n",
        "    excitation = Dense(num_filters, activation='sigmoid')(excitation)\n",
        "    excitation = Reshape((1, 1, num_filters))(excitation)\n",
        "    scale = multiply([input_tensor, excitation])\n",
        "    return scale\n",
        "\n",
        "def unet_model(input_shape, num_filters=64, use_BN=True):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x1, p1 = encoder_block(inputs, num_filters, use_BN)\n",
        "    x1 = SEBlock(x1, num_filters)\n",
        "    x2, p2 = encoder_block(p1, num_filters*2, use_BN)\n",
        "    x2 = SEBlock(x2, num_filters*2)\n",
        "    x3, p3 = encoder_block(p2, num_filters*4, use_BN)\n",
        "    x3 = SEBlock(x3, num_filters*4)\n",
        "    x4, p4 = encoder_block(p3, num_filters*8, use_BN)\n",
        "    x4 = SEBlock(x4, num_filters*8)\n",
        "\n",
        "    # Bridge\n",
        "    bridge = conv_block(p4, num_filters*16, use_BN)\n",
        "    bridge = SEBlock(bridge, num_filters*16)\n",
        "\n",
        "    # Decoder\n",
        "    d1 = decoder_block(bridge, x4, num_filters*8, use_BN)\n",
        "    d2 = decoder_block(d1, x3, num_filters*4, use_BN)\n",
        "    d3 = decoder_block(d2, x2, num_filters*2, use_BN)\n",
        "    d4 = decoder_block(d3, x1, num_filters, use_BN)\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9pwQ2wcudSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InIJbGwH2Bh6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "input_shape = (256, 256, 1)\n",
        "\n",
        "\n",
        "#unet = create_model(input_shape)\n",
        "unet = unet_model(input_shape)\n",
        "lr=1e-3\n",
        "\n",
        "reduce_lr_plateau = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                      factor=0.2,   # Reduction factor (new_lr = lr * factor).\n",
        "                                      patience=15,   # Number of epochs with no improvement after which learning rate will be reduced.\n",
        "                                      min_lr=1e-6,  # Lower bound on the learning rate.\n",
        "                                      verbose=1)\n",
        "\n",
        "optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=25),\n",
        "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),\n",
        "             reduce_lr_plateau\n",
        "             ]\n",
        "\n",
        "\n",
        "#unet.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unet.summary()"
      ],
      "metadata": {
        "id": "mi89AjM3dXQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NNbQhcc5mae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def jaccard_index(y_true, y_pred):\n",
        "    \"\"\"Calculates Jaccard index (IoU) for each image in a batch and averages them.\"\"\"\n",
        "    y_true_f = K.batch_flatten(K.cast(y_true, 'float32'))\n",
        "    y_pred_f = K.batch_flatten(K.cast(y_pred, 'float32'))\n",
        "    intersection = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    union = K.sum(y_true_f, axis=-1) + K.sum(y_pred_f, axis=-1) - intersection\n",
        "    return K.mean((intersection + K.epsilon()) / (union + K.epsilon()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uux9nizHDlht"
      },
      "outputs": [],
      "source": [
        "def power_jaccard_loss(y_true, y_pred, p=2, epsilon=1e-7):\n",
        "    y_true_f = K.batch_flatten(K.cast(y_true, 'float32'))\n",
        "    y_pred_f = K.batch_flatten(K.cast(y_pred, 'float32'))\n",
        "    intersection = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    union = K.sum(K.pow(y_true_f, p), axis=-1) + K.sum(K.pow(y_pred_f, p), axis=-1) - intersection\n",
        "    return 1 - K.mean((intersection + epsilon) / (union + epsilon))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = K.flatten(K.cast(y_true, 'float32'))\n",
        "    y_pred_f = K.flatten(K.cast(y_pred, 'float32'))\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n"
      ],
      "metadata": {
        "id": "9Xv3HEHLnhrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tversky_loss(y_true, y_pred, alpha=0.5, beta=0.5, smooth=1e-6):\n",
        "    y_true_f = K.flatten(K.cast(y_pred, 'float32'))\n",
        "    y_pred_f = K.flatten(K.cast(y_pred, 'float32'))\n",
        "    truepos = K.sum(y_true_f * y_pred_f)\n",
        "    fp_and_fn = alpha * K.sum(y_pred_f * (1 - y_true_f)) + beta * K.sum((1 - y_pred_f) * y_true_f)\n",
        "    return 1 - (truepos + smooth) / (truepos + fp_and_fn + smooth)\n"
      ],
      "metadata": {
        "id": "_mOwUfQEnk_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-q-ZaMMWhOe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "unet = create_model((256, 256, 1))\n",
        "\n",
        "unet.compile(optimizer=optimizer,\n",
        "             loss=tversky_loss,\n",
        "             metrics=[jaccard_index])\n",
        "\n",
        "history = unet.fit(\n",
        "    X_train_augmented, y_train_augmented,\n",
        "    batch_size=8,\n",
        "    epochs=100,\n",
        "    validation_data = (X_test,y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['jaccard_index'])\n",
        "    plt.plot(history.history['val_jaccard_index'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n"
      ],
      "metadata": {
        "id": "i-megVGSlJ2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "def post_process_segmentation(seg_map, threshold=0.5, min_size=100):\n",
        "    \"\"\"\n",
        "    Post-processes the predicted segmentation map.\n",
        "\n",
        "    Parameters:\n",
        "    - seg_map: 2D numpy array, the predicted segmentation map.\n",
        "    - threshold: float, the threshold to apply to the segmentation map to binarize.\n",
        "    - min_size: int, the minimum size of connected components to keep.\n",
        "\n",
        "    Returns:\n",
        "    - processed_seg_map: 2D numpy array, the processed segmentation map.\n",
        "    \"\"\"\n",
        "    # Threshold the segmentation map\n",
        "    seg_map = (seg_map > threshold).astype(np.float32)\n",
        "\n",
        "    # Label connected components\n",
        "    labeled_seg_map, num_features = ndimage.label(seg_map)\n",
        "\n",
        "    # Remove small objects\n",
        "    unique, counts = np.unique(labeled_seg_map, return_counts=True)\n",
        "    for (label, size) in zip(unique, counts):\n",
        "        if size < min_size:\n",
        "            labeled_seg_map[labeled_seg_map == label] = 0\n",
        "\n",
        "    # Recreate binary seg map\n",
        "    processed_seg_map = (labeled_seg_map > 0).astype(np.float32)\n",
        "\n",
        "    return processed_seg_map\n"
      ],
      "metadata": {
        "id": "t2g0g3VhS4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaoSnD8BWhOe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = unet.predict(X_test)\n",
        "y_pred = unet.predict(X_test)\n",
        "\n",
        "# Apply post-processing to each prediction in the batch\n",
        "y_pred_processed = np.array([post_process_segmentation(pred, threshold=0.5, min_size=100) for pred in y_pred])\n",
        "\n",
        "# Convert predictions to binary format\n",
        "y_pred_binary = (y_pred_processed > 0.5).astype(np.float32)\n",
        "\n",
        "# Calculate Jaccard Index for each image in the test set\n",
        "jaccard_values = np.array([jaccard_index(y_test[i], y_pred_binary[i]) for i in range(len(y_test))])\n",
        "jaccard_indices = K.eval(jaccard_values)\n",
        "\n",
        "# Compute median Jaccard Index\n",
        "median_jaccard = np.median(jaccard_indices)\n",
        "print(f\"Median Jaccard Index: {median_jaccard}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl9RXLg6ZTGp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select the first image from X_test\n",
        "sample_index = 33\n",
        "sample_image = X_test[sample_index]\n",
        "\n",
        "true_label = y_test[sample_index]\n",
        "predicted_label = y_pred_binary[sample_index]\n",
        "\n",
        "# Plot the original image\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(131)\n",
        "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the true label\n",
        "plt.subplot(132)\n",
        "plt.imshow(true_label.squeeze(), cmap='gray')\n",
        "plt.title(\"True Label\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the predicted label\n",
        "plt.subplot(133)\n",
        "plt.imshow(predicted_label.squeeze(), cmap='gray')\n",
        "plt.title(\"Predicted Label\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5rWtR9PzhOc"
      },
      "source": [
        "### Real Test predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH6NsnS82970"
      },
      "outputs": [],
      "source": [
        "real_test_data = load_zipped_pickle(\"/content/drive/MyDrive/data/test_processed.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oETXSEMw8JoA"
      },
      "outputs": [],
      "source": [
        "real_test_data_not_processed = load_zipped_pickle(\"/content/drive/MyDrive/data/test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJp3i1D7zjmA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for processed_entry, original_entry in zip(real_test_data, real_test_data_not_processed):\n",
        "    video_frames = processed_entry['video']\n",
        "    num_frames = video_frames.shape[2]\n",
        "    prediction_frames = []\n",
        "\n",
        "    original_height, original_width = original_entry['video'].shape[:2]\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame = video_frames[:, :, i]\n",
        "        frame_reshaped = np.expand_dims(frame, axis=0)\n",
        "        frame_reshaped = np.expand_dims(frame_reshaped, axis=-1)\n",
        "\n",
        "        # Predict mask\n",
        "        pred_mask = unet.predict(frame_reshaped)\n",
        "        pred_binary_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        pred_resized = cv2.resize(pred_binary_mask[0, :, :, 0], (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
        "        pred_resized = pred_resized.astype(np.bool)\n",
        "        prediction_frames.append(pred_resized)\n",
        "\n",
        "    prediction = np.stack(prediction_frames, axis=-1)\n",
        "\n",
        "    # Data Structure\n",
        "    predictions.append({\n",
        "        'name': processed_entry['name'],\n",
        "        'prediction': prediction\n",
        "    })\n",
        "\n",
        "# Save predictions in the correct format\n",
        "save_zipped_pickle(predictions, 'predictions_unet_se.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3ppsQfWdaSh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_image_and_mask(image, mask):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the original image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot the original image with mask overlay\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.imshow(mask, cmap='jet', alpha=0.5)  # Overlay with transparency\n",
        "    plt.title(\"Image with Mask Overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Select video 10 and image 1\n",
        "video_index = 15\n",
        "image_index = 2\n",
        "\n",
        "# Access the original frame and the predicted mask\n",
        "original_frame = real_test_data_not_processed[video_index]['video'][:, :, image_index]\n",
        "predicted_mask = predictions[video_index]['prediction'][:, :, image_index]\n",
        "\n",
        "# Plotting the original image and its mask\n",
        "plot_image_and_mask(original_frame, predicted_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNBXdz5F5EiX"
      },
      "outputs": [],
      "source": [
        "#sample_data = load_zipped_pickle(\"/content/drive/MyDrive/data/sample.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}